{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Case Study 4.1 - Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR NAME              = Qi Guo\n",
    "# YOUR MITX PRO USERNAME = guoqi228\n",
    "# YOUR MITX PRO E-MAIL   = guoqi228@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells to install all the packages you need to complete the remainder of the case study. This may take a few minutes, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting surprise\n",
      "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
      "Collecting scikit-surprise (from surprise)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/fc/cd4210b247d1dca421c25994740cbbf03c5e980e31881f10eaddf45fdab0/scikit-surprise-1.0.6.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 284kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise) (0.11)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise) (1.14.3)\n",
      "Collecting scipy>=1.0.0 (from scikit-surprise->surprise)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 31.2MB 52kB/s eta 0:00:011  8% |██▉                             | 2.8MB 4.2MB/s eta 0:00:07    34% |███████████                     | 10.7MB 8.7MB/s eta 0:00:03    35% |███████████▎                    | 11.0MB 5.8MB/s eta 0:00:04    35% |███████████▌                    | 11.2MB 4.8MB/s eta 0:00:05    55% |█████████████████▊              | 17.3MB 2.3MB/s eta 0:00:07    62% |████████████████████▏           | 19.6MB 3.5MB/s eta 0:00:04    68% |██████████████████████          | 21.4MB 5.5MB/s eta 0:00:02    83% |██████████████████████████▊     | 26.0MB 5.0MB/s eta 0:00:02    96% |███████████████████████████████ | 30.2MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-surprise->surprise) (1.11.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Running setup.py bdist_wheel for scikit-surprise ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/ec/c0/55/3a28eab06b53c220015063ebbdb81213cd3dcbb72c088251ec\n",
      "Successfully built scikit-surprise\n",
      "Installing collected packages: scipy, scikit-surprise, surprise\n",
      "  Found existing installation: scipy 0.19.1\n",
      "    Uninstalling scipy-0.19.1:\n",
      "      Successfully uninstalled scipy-0.19.1\n",
      "Successfully installed scikit-surprise-1.0.6 scipy-1.1.0 surprise-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you must press **Kernel > Restart.** This allows the installation to take effect. Once you see the blue **Connected/Kernel ready** button in the top right, you are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required tools into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "from surprise import Dataset, SVD, NormalPredictor, BaselineOnly, KNNBasic, NMF\n",
    "from surprise.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MovieLens data. A dialog may pop up saying **\"Dataset ml-100k could not be found. Do you want to download it? [Y/n]\"** Type Y and hit Enter to start the download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ml-100k could not be found. Do you want to download it? [Y/n] y\n",
      "Trying to download dataset from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
      "Done! Dataset ml-100k has been saved to /home/nbuser/.surprise_data/ml-100k\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to get a sense of what the data looks like. Let's create a histogram of all the ratings we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c8d71a1d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c8d77a908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Get the ratings file from the data object\n",
    "# This is just a filename that has all the data stored in it\n",
    "ratings_file = data.ratings_file\n",
    "\n",
    "# 2. Load that table using pandas, a commmon python data loading tool\n",
    "# We set the column names manually here\n",
    "col_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "raw_data = pd.read_table(ratings_file, names=col_names)\n",
    "\n",
    "# 3. Get the rating column\n",
    "ratings = raw_data.rating\n",
    "\n",
    "# 4. Generate a bar plot/histogram of that data\n",
    "ratings.value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">QUESTION 1: DATA ANALYSIS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describe the dataset. How many ratings are in the dataset? How would you describe the distribution of ratings? Is there anything else we should observe? Make sure the histogram is visible in the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rating: 100000\n"
     ]
    }
   ],
   "source": [
    "print('Number of rating: {}'.format(len(ratings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your response here...*\n",
    "#### There are 100,000 ratings in the dataset.\n",
    "#### We can see from the histogram that most of the ratings is distributed around 3, 4 and 5. The number of rating 3 is about 26,000. The number of rating 4 is about 34,000. The number of rating 5 is about 21,000. The total number of rating 3, 4 and 5 is 81,000 which is about 81% of total number of ratings.\n",
    "#### Also we can observe that the bias toward high ratings in the disturibution doesn't mean there are more 'good' moives than 'bad' moives. If we assume the rating are normally distuributed, most of the ratings should be around 2, 3 and 5. Actually, users are more likely to rate a moive they like but not to rate a moive they dislike. That's why when a moive has been rated and the rating is more often to be a high rating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model_random = NormalPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm NormalPredictor on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.5193  1.5235  1.5260  1.5128  1.5169  1.5197  0.0047  \n",
      "Fit time          1.22    1.50    1.38    1.47    1.40    1.39    0.10    \n",
      "Test time         1.91    2.32    1.91    1.70    2.96    2.16    0.45    \n"
     ]
    }
   ],
   "source": [
    "# Train on data using cross-validation with k=5 folds, measuring the RMSE\n",
    "model_random_results = cross_validate(model_random, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: User-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model_user = KNNBasic(sim_options={'user_based': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9838  0.9811  0.9757  0.9808  0.9716  0.9786  0.0044  \n",
      "Fit time          5.48    6.37    6.59    6.05    6.17    6.13    0.37    \n",
      "Test time         45.01   44.23   46.31   50.29   53.49   47.87   3.50    \n"
     ]
    }
   ],
   "source": [
    "# Train on data using cross-validation with k=5 folds, measuring the RMSE\n",
    "# Note, this may have a lot of print output\n",
    "# You can set verbose=False to prevent this from happening\n",
    "model_user_results = cross_validate(model_user, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Item-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model_item = KNNBasic(sim_options={'user_based': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9768  0.9699  0.9809  0.9662  0.9759  0.9739  0.0052  \n",
      "Fit time          9.18    11.67   12.11   8.29    6.67    9.58    2.05    \n",
      "Test time         63.90   64.89   59.74   46.78   40.70   55.20   9.71    \n"
     ]
    }
   ],
   "source": [
    "# Train on data using cross-validation with k=5 folds, measuring the RMSE\n",
    "# Note, this may have a lot of print output\n",
    "# You can set verbose=False to prevent this from happening\n",
    "model_item_results = cross_validate(model_item, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">QUESTION 2: COLLABORATIVE FILTERING MODELS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the results from the user-user and item-item models. How do they compare to each other? How do they compare to our original \"random\" model? Can you provide any intuition as to why the results came out the way they did?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your response here...*\n",
    "#### We can see from the result above that the random model gives us a RMSE (Root Mean Squred Error) of 1.5197 (mean of RMSE 5 splits). The user-based collaborative filtering gives us RMSE of 0.9786 (mean of RMSE of 5 splits). The iterm-based collaborative filtering gives us RMSE of 0.9739  (mean of RMSE of 5 splits). \n",
    "#### The random model has the highest RMSE which means this model performs worest. The user-based collaborative filtring and iterm-based collaborative filtering have significantly improved the result (reducing the RMSE by 0.6). The user-based collaborative filtring and iterm-based collaborative filtering perform almost the same considering their mean of RMSE and standard deviation. These two collabrative filtering model performed better because the model consider user similarity and item similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model object\n",
    "model_matrix = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9422  0.9313  0.9339  0.9400  0.9344  0.9364  0.0041  \n",
      "Fit time          30.65   30.81   31.40   30.92   29.66   30.69   0.57    \n",
      "Test time         0.94    1.65    1.56    1.06    1.44    1.33    0.28    \n"
     ]
    }
   ],
   "source": [
    "# Train on data using cross-validation with k=5 folds, measuring the RMSE\n",
    "# Note, this may take some time (2-3 minutes) to train, so please be patient\n",
    "model_matrix_results = cross_validate(model_matrix, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">QUESTION 3: MATRIX FACTORIZATION MODEL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The matrix factorization model is different from the collaborative filtering models. Briefly describe this difference. Also, compare the RMSE again. Does it improve? Can you offer any reasoning as to why that might be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your response here...*\n",
    "#### The collaborative filtering models uses the similarity of user and item while the matrix factorization uses low rank matrix factorization. \n",
    "#### The mean of RMSE of matrix factorization is 0.9364. The result is significantly improved comparing to the random model and even better than the two collabrative filtering model. The reason is matrix factorization uses proximity to  the user in the latent space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall @ `k`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute the precision and recall for 2 values of `k`: 5 and 10. We have provided some code here to help you do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function that takes in some predictions, a value of `k` and a threshold parameter. This code is adapted from [here](http://surprise.readthedocs.io/en/stable/FAQ.html?highlight=precision#how-to-compute-precision-k-and-recall-k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = dict()\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        current = user_est_true.get(uid, list())\n",
    "        current.append((est, true_r))\n",
    "        user_est_true[uid] = current\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the precision and recall at `k` = 5 and 10 for each of our 4 models. We use 5-fold cross validation again to average the results across the entire dataseat.\n",
    "\n",
    "Please note that this will take some time to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">QUESTION 4: PRECISION/RECALL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the precision and recall, for each of the 4 models, at `k` = 5 and 10. This is 2 x 2 x 4 = 16 numerical values. Do you note anything interesting about these values? Anything differerent from the RMSE values you computed above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your response here...*\n",
    "#### When k = 5, we have following results: <br> NormalPredictor: precision: 0.584, recall: 0.338 <br> KNNBasic: precision: 0.765, recall: 0.457 <br> KNNBasic: precision: 0.816, recall: 0.388 <br> SVD precision: 0.78,  recall  : 0.433\n",
    "#### When k = 10, we have following results: <br> NormalPredictor : precision: 0.585 recall  : 0.428 <br> KNNBasic: precision: 0.74, recall  : 0.591 <br> KNNBasic: precision: 0.79, recall  : 0.534 <br> SVD precision: 0.763, recall  : 0.562\n",
    "\n",
    "#### We can see above results that the random model has the lowest precision and recall in both cases of k=5 and k=10. When k = 5 all the results have low recall. For example, KNNBasic (item based) has the highest precision of 0.816 but the recall is only 0.388. When K = 10, we can see all of the recall has increased. However, the highest precision is all about 79% which become smaller. \n",
    "\n",
    "#### The resulty agrees with RMSE result showing that random model has the worest performance. While item based model, user based model and matrix factorization model have much better and similar performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> k=5, model=NormalPredictor\n",
      ">>> precision: 0.584\n",
      ">>> reccall  : 0.338\n",
      ">>> k=5, model=KNNBasic\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      ">>> precision: 0.765\n",
      ">>> reccall  : 0.457\n",
      ">>> k=5, model=KNNBasic\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      ">>> precision: 0.816\n",
      ">>> reccall  : 0.388\n",
      ">>> k=5, model=SVD\n",
      ">>> precision: 0.78\n",
      ">>> reccall  : 0.433\n",
      ">>> k=10, model=NormalPredictor\n",
      ">>> precision: 0.585\n",
      ">>> reccall  : 0.428\n",
      ">>> k=10, model=KNNBasic\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      ">>> precision: 0.74\n",
      ">>> reccall  : 0.591\n",
      ">>> k=10, model=KNNBasic\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      ">>> precision: 0.79\n",
      ">>> reccall  : 0.534\n",
      ">>> k=10, model=SVD\n",
      ">>> precision: 0.763\n",
      ">>> reccall  : 0.562\n"
     ]
    }
   ],
   "source": [
    "# Make list of k values\n",
    "K = [5, 10]\n",
    "\n",
    "# Make list of models\n",
    "models = [model_random, model_user, model_item, model_matrix]\n",
    "\n",
    "# Create k-fold cross validation object\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for k in K:\n",
    "    for model in models:\n",
    "        print(f'>>> k={k}, model={model.__class__.__name__}')\n",
    "        # Run folder and take average\n",
    "        p = []\n",
    "        r = []\n",
    "        for trainset, testset in kf.split(data):\n",
    "            model.fit(trainset)\n",
    "            predictions = model.test(testset, verbose=False)\n",
    "            precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5)\n",
    "\n",
    "            # Precision and recall can then be averaged over all users\n",
    "            p.append(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "            r.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "        \n",
    "        print('>>> precision:', round(sum(p) / len(p), 3))\n",
    "        print('>>> reccall  :', round(sum(r) / len(r), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Top-`n` Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see what some of the actual movie ratings are for particular users, as outputs of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define a helpful function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=5):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = dict()\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        current = top_n.get(uid, [])\n",
    "        current.append((iid, est))\n",
    "        top_n[uid] = current\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we call this function on each of our models, first training on **all** the data we have available, then predicting on the remaining, missing data. We use `n`=5 here, but you can pick any reasonable value of `n` you would like.\n",
    "\n",
    "This may take some time to compute, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = data.build_full_trainset()\n",
    "testset = trainset.build_anti_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">QUESTION 5: TOP N PREDICTIONS</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do the top n predictions that you received make sense? What is the rating value (1-5) of these predictions? How could you use these predictions in the real-world if you were trying to build a generic content recommender system for a company?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your response here...*\n",
    "#### Random model top 5 prediction: '40', '1042', '277', '979', '332'.  <br> User based model top 5 pridiction: '1189', '1500', '814', '1536', '1599'. <br> Item based model top 5 prediction: '1414', '1309', '1310', '1675', '1676'. Matrix model top 5 prediction: '515', '357', '318', '114', '98'. \n",
    "\n",
    "#### In real-world, we could push all the recommended contents to user. Base on user's  feedback on the recommended contents, we can furthermore calibrate our models to make better prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: <surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x7f1c95a47358>, 196: [('40', 5), ('1042', 5), ('277', 5), ('979', 5), ('332', 5)]\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "model: <surprise.prediction_algorithms.knns.KNNBasic object at 0x7f1c8d6d6b38>, 196: [('1189', 5), ('1500', 5), ('814', 5), ('1536', 5), ('1599', 5)]\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "model: <surprise.prediction_algorithms.knns.KNNBasic object at 0x7f1c8d6d6c18>, 196: [('1414', 4.666666666666667), ('1309', 4.5), ('1310', 4.5), ('1675', 4.333333333333333), ('1676', 4.3076923076923075)]\n",
      "model: <surprise.prediction_algorithms.matrix_factorization.SVD object at 0x7f1c8d73b6a0>, 196: [('515', 4.648483393588845), ('357', 4.6459641502344535), ('318', 4.621387641080771), ('114', 4.592982684972789), ('98', 4.552627600522945)]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.fit(trainset)\n",
    "    predictions = model.test(testset)\n",
    "    top_n = get_top_n(predictions, n=5)\n",
    "    # Print the first one\n",
    "    user = list(top_n.keys())[0]\n",
    "    print(f'model: {model}, {user}: {top_n[user]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Now, make sure you check out the **Conclusion** section of the [instruction manual](https://courses.edx.org/asset-v1:MITxPRO+DSx+2T2018+type@asset+block@4.1_instruction_manual.html) to wrap up this case study properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
